---
title: "Multiple_Regression"
author: "Veera_Namana"
date: "9/17/2019"
output: pdf_document
---

* The dataset has 506 rows and 14 columns.

* The column/variable names' explanation is given below:

1) __CRIM :__ Per capita Crime rate by town

2) __ZN :__ Proportion of residential land zoned for lots over 25,000 sq.ft.

3) __INDUS :__ Proportion of non-retail business acres per town

4) __CHAS :___ Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)

5) __NOX :__ nitric oxides concentration (parts per 10 million)

6) __RM :__ average number of rooms per dwelling

7) __AGE :__ proportion of owner-occupied units built prior to 1940

8) __DIS :__ weighted distances to five Boston employment centres

9) __RAD :__ index of accessibility to radial highways

10) __TAX :__ full-value property-tax rate per $10,000

11) __PTRATIO :__ pupil-teacher ratio by town

12) __B :__ 1000(Bk - 0.63)^2 where Bk is the proportion of African-Americans by town

13) __LSTAT :__ Percentage of the population in the lower economic status 

14) __MEDV  :__ Median value of owner-occupied homes in multiples of $1000


* Take a look at the data using the "head()" and "tail()" functions

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Clearing the environment
```{R, echo =T}

rm(list = ls(all=TRUE))

```


#Setting up working directory and reading the data
```{R, echo =T}

getwd()

setwd("/Users/tejaswinamana/downloads/practice")

housing_data = read.csv("HousingData.csv", header = TRUE)

```

#structure of housing data
```{R, echo =T}

str(housing_data)

```

#
```{R, echo =T}

head(housing_data)

tail(housing_data)

```

#Summary statistics
```{R, echo =T}

summary(housing_data)

```

#Identifying NA values
```{R, echo =T}
#Total NA values in the dataset

sum(is.na(housing_data))       #70


#how to get columns wise count of null values

sum(is.na(housing_data$CRIM))   #7
sum(is.na(housing_data$ZN))     #7
sum(is.na(housing_data$INDUS))  #4
sum(is.na(housing_data$CHAS))   #5
sum(is.na(housing_data$NOX))    #6
sum(is.na(housing_data$RM))     #4
sum(is.na(housing_data$AGE))    #3
sum(is.na(housing_data$DIS))    #4
sum(is.na(housing_data$RAD))    #5
sum(is.na(housing_data$TAX))    #6
sum(is.na(housing_data$PT))     #9
sum(is.na(housing_data$B))      #4
sum(is.na(housing_data$LSTAT))  #6
sum(is.na(housing_data$MV))     #0
```

#Scatter Plots

```{R, echo =T}

par(mfrow = c(2,2))

plot(housing_data$CRIM,housing_data$MV,
     xlab = "Per Capita Crime Rate",
     ylab = "Median House Price",
     main = "Housing Price vs. Crime Rate")

plot(housing_data$ZN,housing_data$MV,
     xlab = "Residential Land Zoned",
     ylab = "Median House Price",
     main = "Housing Price vs. Land Zone")

plot(housing_data$INDUS,housing_data$MV,
     xlab = "Non Retail Business Acres",
     ylab = "Median House Price",
     main = "Housing Price vs. Non Retail Business")

plot(housing_data$CHAS,housing_data$MV,
     xlab = "Charles River",
     ylab = "Median House Price",
     main = "Housing Price vs. Charles River")

plot(housing_data$NOX,housing_data$MV,
     xlab = "Nitric Oxides Concentration",
     ylab = "Median House Price",
     main = "Housing Price vs. NOX concentration")

plot(housing_data$RM,housing_data$MV,
     xlab = "Number of rooms per dwelling",
     ylab = "Median House Price",
     main = "Housing Price vs. Dwelling Rooms")

plot(housing_data$AGE,housing_data$MV,
     xlab = "Owner occupied Rooms (before 1940)",
     ylab = "Median House Price",
     main = "Housing Price vs. Owner occupied rooms")

plot(housing_data$DIS,housing_data$MV,
     xlab = "Distance to Employment Centers",
     ylab = "Median House Price",
     main = "Housing Price vs. Distance")

plot(housing_data$RAD,housing_data$MV,
     xlab = "Accessability to Radial Highways",
     ylab = "Medin House Price",
     main = "Housing Price vs. Radial Highways")

plot(housing_data$TAX,housing_data$MV,
     xlab = "Property Tax Rate",
     ylab = "Median House Price",
     main = "Housing Price vs. property tax rate")

plot(housing_data$PT,housing_data$MV,
     xlab = "Pupil Teacher Ratio",
     ylab = "Median House Price",
     main = "Housing Price vs. Pupil Teacher Ratio")

plot(housing_data$B,housing_data$MV,
     xlab = "African American Proportion",
     ylab = "Medin House Price",
     main = "Housing Price vs. African American Proportion")

plot(housing_data$LSTAT,housing_data$MV,
     xlab = "Lower economic status population",
     ylab = "Medin House Price",
     main = "Housing Price vs. Lower economic status population")

```
#Housing Price vs Nox concentration shows negative correlation
#Housing Price vs Dwelling room show positive correlation
#Housing price vs lower economic status population shows negative correlation

#Correlation Plot
```{R, echo =T}

library(corrplot)

cor(housing_data, use = "complete.obs")

corrplot(cor(housing_data,use = "complete.obs"), method = "number")

```

#Impute missing values
```{R, echo =T}
#library(DMwR)

#housing_data_Imputed = centralImputation(housing_data)

```


#Missing values(NA) are replaced with means

#Checking categorical and continuous variables
```{R, echo =T}
#Continuous
#table(housing_data$CRIM)
#length(unique(housing_data$CRIM))

#numeric
#table(housing_data$ZN)
#length(unique(housing_data$ZN))

#numeric
#table(housing_data$INDUS)
#length(unique(housing_data$INDUS))

#Categorical
#table(housing_data_Imputed$CHAS)
#length(unique(housing_data_Imputed$CHAS))

#Continuous
#table(housing_data_Imputed$NOX)
#length(unique(housing_data_Imputed$NOX))

#Continuous
#table(housing_data_Imputed$RM)
#length(unique(housing_data_Imputed$RM))

#Continuous
#table(housing_data_Imputed$AGE)
#length(unique(housing_data_Imputed$AGE))

#Continuous
#table(housing_data_Imputed$DIS)
#length(unique(housing_data_Imputed$DIS))

#Categorical
#table(housing_data_Imputed$RAD)
#length(unique(housing_data_Imputed$RAD))

#Numeric
#table(housing_data_Imputed$TAX)
#length(unique(housing_data_Imputed$TAX))

#Continuous
#table(housing_data_Imputed$PT)
#length(unique(housing_data_Imputed$PT))
```

#Converting into categorical variables
```{R, echo =T}

housing_data$CHAS <- as.factor(housing_data$CHAS)
#housing_data_Imputed$RAD <- as.factor(housing_data_Imputed$RAD)

str(housing_data)
```

#Train/Test Split -70/30

```{R, echo =T}
set.seed(123)

train_rows = sample(1:nrow(housing_data), 0.7*nrow(housing_data))

train_data <- housing_data[train_rows,]
test_data <- housing_data[-train_rows,]

dim(train_data)
dim(test_data)

```

#Column wise missing data 
```{R, echo =T}

cat("Missing values in Train Data \n")
colSums(is.na(train_data))

cat("Missing values in Test Data \n")
colSums(is.na(test_data))

```

#Missing values imputation
```{R, echo =T}

library(caret)

#Before Imputation
sum(is.na(train_data))
sum(is.na(test_data))

imputer_values <- preProcess(x=train_data[,!names(train_data) %in% c("CHAS")], METHOD = "medianImpute")

train_data[,!names(train_data) %in% c("CHAS")] <- predict(object = imputer_values,newdata = train_data[,!names(train_data) %in% c("CHAS")])
sum(is.na(train_data[,!names(train_data) %in% c("CHAS")]))


test_data[,!names(train_data) %in% c("CHAS")] <- predict(object = imputer_values, newdata = test_data[,!names(train_data) %in% c("CHAS")])
sum(is.na(test_data[,!names(train_data) %in% c("CHAS")]))

#After imputation
sum(is.na(train_data))
sum(is.na(test_data))
```

## Imputing the Categorical attributes with the Model
```{R, echo =T}

library(DMwR)
centralValue(train_data$CHAS)
train_data=centralImputation(train_data)
test_data = centralImputation(test_data)

```
#Checking for missing values in both train and test
```{R, echo =T}

cat("Missing values in Train data after imputation")
colSums(is.na(train_data))

cat("Missing values in Test data after imputation")
colSums(is.na(test_data))

```

#Standardizing the data
```{R, echo =T}

std_model <- preProcess(train_data[,!names(train_data) %in% c("MV","CHAS")], method = c("center","scale"))

std_model

train_data[,!names(train_data) %in% c("MV","CHAS")] <- predict(object = std_model, newdata=train_data[,!names(train_data) %in% c("MV","CHAS")])

test_data[,!names(train_data) %in% c("MV","CHAS")] <- predict(object = std_model, newdata=test_data[,!names(train_data) %in% c("MV","CHAS")])

summary(train_data)
```

#Modelling the data
```{R, echo =T}
model_basic<- lm(MV~., data = train_data)

```

#Model Summary
```{R, echo =T}

summary(model_basic)

```
#* Question: How good is the model?

# 74% of variation in price of the home is explained with the given independent variables like crime rate, pupil teacher ratio etc.,

#* Question: Is the model significant?
# Yes, model is significant(p value<0.05).
# There are some iindependent variables that are significant and some are not


#Residuals
```{R, echo =T}

df_residuals <- model_basic$residuals

```

```{R, echo =T}

Residuals = function(model, data,target){
  predicted = predict(model,data)
  residuals = target-predicted
  return(residuals)
}
dfResiduals = Residuals(model_basic, train_data,train_data$MV)

```

```{R, echo =T}

summary(dfResiduals)

```

#Residual plots
```{R, echo =T}

par(mfrow =c(2,2))
plot(model_basic)

```
#From the residual plots we can say the data is homoscedastic
#From the Q-Q Plot we can see error terms are normally distributed
#There are no influential points as per cook's distance


#Influential observations
#Cook's distance
#Identifying influential observations and handling them
```{R, echo =T}

cook = cooks.distance(model_basic)
plot(cook, ylab = "cook's distances")
max = as.numeric(which.max(cook))
max
points(max,cook[max], col="red", pch=19)
text(seq(1:nrow(train_data)), cook, labels = rownames(train_data))

train_cook <- train_data[-max,]
dim(train_cook)

```

#Model Building after removing influentiial observations

#Only inluded significant variables
```{R, echo =T}

model_basic2 <- lm(MV~.,data = train_cook)
summary(model_basic2)

par(mfrow=c(2,2))

plot(model_basic2)

```
#Adjusted R^2 did not change much


#stepAIC model
```{R, echo =T}

library(MASS)

model_aic <- stepAIC(model_basic2, direction = "both", trace = 1)
model_aic
summary(model_aic)

par(mfrow=c(2,2))

plot(model_aic)
```

#Variance Inflaton Factor
```{R, echo =T}

library(car)

vif(model_basic)

vif(model_aic)

```
#After applying the stepAIC, the VIF values have slightly reduced, but the variables "RAD" and "TAX" have VIF values higher than 4

#Correltion between RAD and TAX variables
```{R, echo =T}

cor(housing_data$RAD,housing_data$TAX, use = "complete.obs")

```
#* The correlation coefficient is extremely high between the "RAD" and "TAX" variables
#* let's now remove the "TAX" variable, as it is the lesser significant of the two
#* Build another model without the "TAX" variable, and take a look at the VIF
```{R, echo =T}

model3 <- lm(formula = MV ~ CRIM + ZN + CHAS + NOX + RM + DIS + RAD + PT + B + LSTAT, data = train_data)

summary(model3)

par(mfrow=c(2,2))

plot(model3)

vif(model3)

```

# Evaluation and Selection of Model

## Picking the Right Model

#* The third model built after verifying the vif scores has a similar adjusted R^2 score compared to the previous models with significantly lower no. of explanatory variables and inter-variable interactions.

#* The VIF values of the predictors in the third model are lower when compared to the the other two models

#* Due to the above two reasons we pick the third model


#communication

#Prediction
```{R, echo =T}

preds_model <- predict(model3, test_data[,!(names(test_data) %in% c("MV"))])

preds_model

```


###Report Performance Metris
```{R, echo =T}

library(DMwR)

regr.eval(train_data$MV,model3$fitted.values)

```

#Error verification on test data
```{R, echo =T}

regr.eval(test_data$MV,preds_model)

```
#Observation: Error Metrics is not far apart for train and test data after its evaluated on the third model

#* Future Scope: 
#  * i) More models can be tried to improve Adjusted R^2
#  * ii) Data Transformation can be applied 
